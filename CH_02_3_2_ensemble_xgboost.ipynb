{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning - 2\n",
    "***\n",
    "\n",
    "지난 챕터에서는 앙상블 알고리즘 중 하나인 RandomForest에 대해 알아보았는데요. 이번 챕터에서는 부스팅 계열의 알고리즘에 대해 살펴보도록 하겠습니다. 먼저 부스팅 알고리즘은 여러 개의 약한 학습기(weak learner)를 학습 및 예측해가면서 잘못 예측된 데이터에 대해 가중치를 부여하여 오류를 개선해가면서 학습하는 방식입니다. \n",
    "\n",
    "대표적인 부스팅 AdaBoost와 GradientBoost가 있습니다. \n",
    "\n",
    "먼저 AdaBoost는 Adaptive + Boosting으로 이전 분류기가 잘못 예측한 데이터의 가중치를 adaptive하게 수정해가며 잘못 예측된 데이터에 가중치를 부여하고 더 집중하여 오류를 개선해나갑니다.\n",
    "\n",
    "반면 GradientBoost는 AdaBoost와 유사하지만 가중치 업데이트를 경사하강법(Gradient Descent)을 활용합니다. 실제 값과 예측 값의 차이를 최소로 줄일 수 있는 방향으로 반복적으로 가중치를 업데이트 하는 방식을 통해 데이터를 학습하고 예측하게 됩니다.\n",
    "***\n",
    "바로 실습을 통해서 다른 모델들과 비교를 해가면서 자세히 들여다 보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression() # 로지스틱 회귀 모델\n",
    "ada_clf = AdaBoostClassifier(random_state = 1212) # 에이다부스트 모델\n",
    "gbm_clf = GradientBoostingClassifier(random_state = 1212) # 그래디언트부스트 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size = .2, random_state = 1212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(random_state=1212)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf.fit(X_train, y_train)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "gbm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_acc = np.round(accuracy_score(y_test,log_clf.predict(X_test)), 6) * 100\n",
    "ada_acc = np.round(accuracy_score(y_test, ada_clf.predict(X_test)), 4) * 100\n",
    "gbm_acc = np.round(accuracy_score(y_test, gbm_clf.predict(X_test)), 4) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression의 정확도는 94.7368%\n",
      "AdaBoostClassifier의 정확도는 96.49%\n",
      "GradientBoostingClassifier의 정확도는 95.61%\n"
     ]
    }
   ],
   "source": [
    "print(f'{log_clf.__class__.__name__}의 정확도는 {log_acc}%')\n",
    "print(f'{ada_clf.__class__.__name__}의 정확도는 {ada_acc}%')\n",
    "print(f'{gbm_clf.__class__.__name__}의 정확도는 {gbm_acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 로지스틱 회귀 모델 보다 성능이 좋은 것을 확인하였습니다. \n",
    "\n",
    "부스팅 계열도 트리 기반의 알고리즘이기 때문에 하이퍼 파라미터는 트리 알고리즘이 가지는 하이퍼 파라미터는 기본적으로 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'n_estimators' : [20, 30, 50, 100 ,300 ,500], 'max_depth' : [3,4,5,6,7], \n",
    "        'learning_rate' : [0.005, 0.01, 0.05, 0.1, 0.2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_clf = GradientBoostingClassifier(random_state = 1212)\n",
    "\n",
    "grid_gbm = GridSearchCV(gbm_clf, param, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=GradientBoostingClassifier(random_state=1212), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.005, 0.01, 0.05, 0.1, 0.2],\n",
       "                         'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'n_estimators': [20, 30, 50, 100, 300, 500]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_gbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 파라미터 조합은 {'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 100}\n",
      "최고 정확도는 96.92%\n"
     ]
    }
   ],
   "source": [
    "print('최적의 파라미터 조합은',grid_gbm.best_params_)\n",
    "print(f'최고 정확도는 {np.round(grid_gbm.best_score_, 4) * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 파라미터를 가지고 최종 예측을 해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.2, max_depth=4, random_state=1212)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_gbm_clf = GradientBoostingClassifier(**grid_gbm.best_params_, random_state = 1212)\n",
    "final_gbm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_acc = np.round(accuracy_score(y_test, final_gbm_clf.predict(X_test)), 4) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 GradientBoostingClassifier의 정확도는 97.37%\n"
     ]
    }
   ],
   "source": [
    "print(f'최종 {final_gbm_clf.__class__.__name__}의 정확도는 {final_acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoosting 모델은 과적합에도 덜 취약한 알고리즘이지만 수행 시간이 오래 걸린다는 단점이 있는데요. 이러한 단점을 보완하고 현재 ML에서 제일 널리 사용되고 있는 XGBoost와 LightGBM에 대해서 알아보도록 하겠습니다.\n",
    "***\n",
    "\n",
    "XGBoost(eXtra Gradient Boost)는 아래와 같은 장점을 가지고 있습니다.\n",
    "\n",
    " \n",
    "| 장점 | 설명 |\n",
    "|:--------|:--------:|\n",
    "| 뛰어난 성능  | 분류와 회귀에서 뛰어난 성능을 보여준다. |\n",
    "| 빠른 수행 시간 | GradientBoosting의 느린 수행 시간 보완 |\n",
    "| 과적합 규제 | 과적합에 좀 더 강함 |\n",
    "| 나무 가지치기 | 가지치기로 분할 수를 감소 시킨다. |\n",
    "| 자체 교차 검증 가능 | 교차 검증 및 조기 중단 기능 내장 |\n",
    "| 결측치 자체 처리 | 자체적으로 결측치 처리 가능 |\n",
    "\n",
    "***\n",
    "XGBoost는 XGBoost API를 사용하는 방법과 sklearn 전용 방법 두 가지가 있는데 XGBoost API를 먼저 사용해보도록하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost는 전용 API 사용시 DMatrix 객체를 생성하여 사용합니다. DMatrix는 Numpy 배열을 입력으로 받아 만들어지는 XGBoost 전용 데이터 포맷입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data = X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(data = X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 구성한 후 train() 메서드를 사용하여 학습을 진행합니다. XGBoost에는 조기중단 기능이 제공되는데 조기 중단 기능은 학습시에 성능이 개선되지 않을 경우 학습을 종료하는 기능입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.36225\ttest-rmse:0.38009\n",
      "[1]\ttrain-rmse:0.26756\ttest-rmse:0.30057\n",
      "[2]\ttrain-rmse:0.19704\ttest-rmse:0.25099\n",
      "[3]\ttrain-rmse:0.14981\ttest-rmse:0.22035\n",
      "[4]\ttrain-rmse:0.11599\ttest-rmse:0.20590\n",
      "[5]\ttrain-rmse:0.09390\ttest-rmse:0.20095\n",
      "[6]\ttrain-rmse:0.07849\ttest-rmse:0.19697\n",
      "[7]\ttrain-rmse:0.06989\ttest-rmse:0.19453\n",
      "[8]\ttrain-rmse:0.06111\ttest-rmse:0.19250\n",
      "[9]\ttrain-rmse:0.05696\ttest-rmse:0.19268\n",
      "[10]\ttrain-rmse:0.04995\ttest-rmse:0.19177\n",
      "[11]\ttrain-rmse:0.04598\ttest-rmse:0.19176\n",
      "[12]\ttrain-rmse:0.04297\ttest-rmse:0.19165\n",
      "[13]\ttrain-rmse:0.03933\ttest-rmse:0.19168\n",
      "[14]\ttrain-rmse:0.03500\ttest-rmse:0.19177\n",
      "[15]\ttrain-rmse:0.03156\ttest-rmse:0.19168\n",
      "[16]\ttrain-rmse:0.03022\ttest-rmse:0.19172\n",
      "[17]\ttrain-rmse:0.02799\ttest-rmse:0.19195\n",
      "[18]\ttrain-rmse:0.02698\ttest-rmse:0.19204\n",
      "[19]\ttrain-rmse:0.02519\ttest-rmse:0.19105\n",
      "[20]\ttrain-rmse:0.02477\ttest-rmse:0.19113\n",
      "[21]\ttrain-rmse:0.02440\ttest-rmse:0.19110\n",
      "[22]\ttrain-rmse:0.02268\ttest-rmse:0.19103\n",
      "[23]\ttrain-rmse:0.02137\ttest-rmse:0.19106\n",
      "[24]\ttrain-rmse:0.02007\ttest-rmse:0.19148\n",
      "[25]\ttrain-rmse:0.01951\ttest-rmse:0.19130\n",
      "[26]\ttrain-rmse:0.01792\ttest-rmse:0.19100\n",
      "[27]\ttrain-rmse:0.01732\ttest-rmse:0.19087\n",
      "[28]\ttrain-rmse:0.01660\ttest-rmse:0.19078\n",
      "[29]\ttrain-rmse:0.01639\ttest-rmse:0.19069\n",
      "[30]\ttrain-rmse:0.01543\ttest-rmse:0.19062\n",
      "[31]\ttrain-rmse:0.01489\ttest-rmse:0.19068\n",
      "[32]\ttrain-rmse:0.01434\ttest-rmse:0.19072\n",
      "[33]\ttrain-rmse:0.01409\ttest-rmse:0.19066\n",
      "[34]\ttrain-rmse:0.01324\ttest-rmse:0.19062\n",
      "[35]\ttrain-rmse:0.01302\ttest-rmse:0.19066\n",
      "[36]\ttrain-rmse:0.01260\ttest-rmse:0.19047\n",
      "[37]\ttrain-rmse:0.01227\ttest-rmse:0.19044\n",
      "[38]\ttrain-rmse:0.01192\ttest-rmse:0.19031\n",
      "[39]\ttrain-rmse:0.01170\ttest-rmse:0.19026\n",
      "[40]\ttrain-rmse:0.01120\ttest-rmse:0.19025\n",
      "[41]\ttrain-rmse:0.01097\ttest-rmse:0.19025\n",
      "[42]\ttrain-rmse:0.01019\ttest-rmse:0.19022\n",
      "[43]\ttrain-rmse:0.00963\ttest-rmse:0.19037\n",
      "[44]\ttrain-rmse:0.00915\ttest-rmse:0.19083\n",
      "[45]\ttrain-rmse:0.00902\ttest-rmse:0.19081\n",
      "[46]\ttrain-rmse:0.00868\ttest-rmse:0.19081\n",
      "[47]\ttrain-rmse:0.00835\ttest-rmse:0.19070\n",
      "[48]\ttrain-rmse:0.00808\ttest-rmse:0.19067\n",
      "[49]\ttrain-rmse:0.00796\ttest-rmse:0.19063\n",
      "[50]\ttrain-rmse:0.00743\ttest-rmse:0.19037\n",
      "[51]\ttrain-rmse:0.00733\ttest-rmse:0.19044\n",
      "[52]\ttrain-rmse:0.00707\ttest-rmse:0.19042\n",
      "[53]\ttrain-rmse:0.00683\ttest-rmse:0.19047\n",
      "[54]\ttrain-rmse:0.00652\ttest-rmse:0.19047\n",
      "[55]\ttrain-rmse:0.00641\ttest-rmse:0.19048\n",
      "[56]\ttrain-rmse:0.00625\ttest-rmse:0.19055\n",
      "[57]\ttrain-rmse:0.00616\ttest-rmse:0.19058\n",
      "[58]\ttrain-rmse:0.00588\ttest-rmse:0.19055\n",
      "[59]\ttrain-rmse:0.00564\ttest-rmse:0.19051\n",
      "[60]\ttrain-rmse:0.00551\ttest-rmse:0.19049\n",
      "[61]\ttrain-rmse:0.00523\ttest-rmse:0.19037\n",
      "[62]\ttrain-rmse:0.00505\ttest-rmse:0.19039\n",
      "[63]\ttrain-rmse:0.00478\ttest-rmse:0.19038\n",
      "[64]\ttrain-rmse:0.00450\ttest-rmse:0.19031\n",
      "[65]\ttrain-rmse:0.00443\ttest-rmse:0.19033\n",
      "[66]\ttrain-rmse:0.00425\ttest-rmse:0.19037\n",
      "[67]\ttrain-rmse:0.00419\ttest-rmse:0.19031\n",
      "[68]\ttrain-rmse:0.00415\ttest-rmse:0.19031\n",
      "[69]\ttrain-rmse:0.00406\ttest-rmse:0.19028\n",
      "[70]\ttrain-rmse:0.00391\ttest-rmse:0.19030\n",
      "[71]\ttrain-rmse:0.00372\ttest-rmse:0.19031\n",
      "[72]\ttrain-rmse:0.00355\ttest-rmse:0.19022\n",
      "[73]\ttrain-rmse:0.00338\ttest-rmse:0.19021\n",
      "[74]\ttrain-rmse:0.00332\ttest-rmse:0.19019\n",
      "[75]\ttrain-rmse:0.00328\ttest-rmse:0.19019\n",
      "[76]\ttrain-rmse:0.00313\ttest-rmse:0.19021\n",
      "[77]\ttrain-rmse:0.00305\ttest-rmse:0.19021\n",
      "[78]\ttrain-rmse:0.00297\ttest-rmse:0.19021\n",
      "[79]\ttrain-rmse:0.00286\ttest-rmse:0.19020\n",
      "[80]\ttrain-rmse:0.00271\ttest-rmse:0.19018\n",
      "[81]\ttrain-rmse:0.00268\ttest-rmse:0.19017\n",
      "[82]\ttrain-rmse:0.00263\ttest-rmse:0.19015\n",
      "[83]\ttrain-rmse:0.00252\ttest-rmse:0.19018\n",
      "[84]\ttrain-rmse:0.00243\ttest-rmse:0.19014\n",
      "[85]\ttrain-rmse:0.00233\ttest-rmse:0.19013\n",
      "[86]\ttrain-rmse:0.00227\ttest-rmse:0.19012\n",
      "[87]\ttrain-rmse:0.00217\ttest-rmse:0.19011\n",
      "[88]\ttrain-rmse:0.00209\ttest-rmse:0.19008\n",
      "[89]\ttrain-rmse:0.00201\ttest-rmse:0.19008\n",
      "[90]\ttrain-rmse:0.00196\ttest-rmse:0.19009\n",
      "[91]\ttrain-rmse:0.00187\ttest-rmse:0.19008\n",
      "[92]\ttrain-rmse:0.00184\ttest-rmse:0.19008\n",
      "[93]\ttrain-rmse:0.00182\ttest-rmse:0.19007\n",
      "[94]\ttrain-rmse:0.00175\ttest-rmse:0.19006\n",
      "[95]\ttrain-rmse:0.00171\ttest-rmse:0.19005\n",
      "[96]\ttrain-rmse:0.00168\ttest-rmse:0.19005\n",
      "[97]\ttrain-rmse:0.00161\ttest-rmse:0.19002\n",
      "[98]\ttrain-rmse:0.00155\ttest-rmse:0.19001\n",
      "[99]\ttrain-rmse:0.00150\ttest-rmse:0.19004\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = xgb.train(params = {'max_depth' : 4}, dtrain = dtrain, num_boost_round = 100, evals = [(dtrain, 'train'), (dtest, 'test')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb_clf.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost의 API는 predict 메서드 실행시 확률 값을 반환하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0031145 ,  1.0087953 ,  0.7946443 ,  1.0017257 ,  1.0014106 ,\n",
       "        0.9946285 ,  0.99687135,  0.99919975,  0.9931866 , -0.01643419],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:10] # 반환된 확률 10개 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [1 if x > .5 else 0 for x in pred] # 확률 값을 0.5 를 기준으로 binary 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost API 활용시 자체적으로 교차 검증(cross validation)이 가능한데요. xgb.cv()를 활용하여 자체적으로 교차 검증이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_param = {'max_depth' : 6, 'eta' : 0.005, 'objective' : 'binary:logistic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xgb = xgb.cv(params = cv_param, dtrain = dtrain, num_boost_round = 100,\n",
    "               nfold = 5, stratified = True, metrics = 'error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "sklearn의 XGBoost 클래스를 사용하여 작업을 진행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(random_state = 1214, n_estimators = 100,max_depth = 4) # XGBppst API의 'num_boost_round'와 'n_estimators'는 같은 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              random_state=1214, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, pred)) # 위의 XGBoost API 사용 결과와 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn의 XGBoostClassifier도 조기 중단 기능을 사용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              random_state=1214, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.fit(X_train, y_train, early_stopping_rounds = 100, eval_metric = 'logloss', eval_set = [(X_test, y_test)], verbose=0) # verbose = 0이면 과정 출력 생략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'red'>주의!\n",
    "    \n",
    "현재 챕터에서는 모델 학습시 예측해야할 X_test도 학습과정시 검증용 데이터로 사용하였습니다\n",
    "ex> fit(~~~, eval_set = [(X_test, y_test)])\n",
    "하지만 실제 문제 해결시에는 우리가 예측해야할 데이터는 모델링 진행시 절대로 사용하면 안됩니다.\n",
    "    \n",
    "해당 문제는 data leakage의 문제가 있기 때문에 예측해야할 데이터를 사전에 학습하는 누수의 문제점이 있습니다.\n",
    "    \n",
    "### <font color = 'blue'> 따라서 train 데이터만 모델 학습에 사용하고 최종 예측해야할 데이터는 모델링 진행시 사용하시면 안됩니다!\n",
    "    \n",
    "*** \n",
    "지금까지 앙상블 알고리즘 중에서 XGBoost에 대해서 알아보았습니다. 다음 챕터에서는 XGBoost 처럼 ML에서 각광을 받고 있는 Lightgbm에 대해 살펴보도록 하겠습니다. 감사합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
