{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning - 1\n",
    "***\n",
    "이번 챕터에서는 앙상블 학습에 대해서 알아보도록 하겠습니다. '백지장도 맞들면 낫다.'라는 말 많이들 들어보셨을텐데요. 앙상블을 설명하기에 아주 알맞은 말이라고 생각됩니다. 앙상블은 여러 개의 분류기 혹은 모델들의 예측 값들을 결합하여 최종적으로 좀 더 좋은 성능을 내는 기법입니다. '집단지성'도 어찌보면 앙상블과 일맥상통한다고 볼 수 있겠습니다. <br>\n",
    "정형 데이터의 문제 해결에 앙상블 알고리즘이 강세를 보이고 있는데 그 중에서도 부스팅 계열의 알고리즘(xgboost, lightgbm 등)이 뛰어난 성능을 보여주고 있습니다. 이번 챕터에서는 전통적인 앙상블의 유형인 보팅(Voting), 배깅(Bagging), 부스팅(Boosting), 스태킹(Stacking)에 대해서 알아보도록 하겠습니다.<br>\n",
    "\n",
    "- 보팅(Voting) : 서로 다른 알고리즘을 활용하여 예측한 결과값을 바탕으로 최종예측\n",
    "- 부스팅(Boosting) : 예측이 틀린 데이터는 다른 분류기가 학습시 가중치 부여하여 학습 및 예측 진행\n",
    "\n",
    "- 스태킹(Stacking) : 서로 다른 모델의 예측 결과값을 다시 학습데이터로 사용하여 다른 모델로 재학습 및 예측 진행\n",
    "\n",
    "- 배깅(Bootstrap Aggregating의 줄임말) : train 데이터에서 중복을 허용하여 샘플링하는 방식을 통해 같은 분류기내에서 서로 다른 데이터로 학습\n",
    "\n",
    "     cf) 페이스팅(Pasting) : 중복을 허용하지 않고 샘플링\n",
    "<br>\n",
    "***\n",
    "    \n",
    "먼저 voting classifier부터 살펴보도록 하겠습니다. 데이터는 사이킷런에 내장되어 있는 위스콘신 유방암 데이터를 활용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "df = pd.DataFrame(cancer.data, columns = cancer.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 구성한 후 '로지스틱회귀'와 'KNN'을 기반으로 '소프트 보팅'방식을 활용해 새로운 모델을 만들어보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size = .2, random_state = 1212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression() # 로지스틱 회귀 모델\n",
    "knn_clf = KNeighborsClassifier() # KNN 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf.fit(X_train, y_train) # 로지스틱 회귀 모델 학습\n",
    "knn_clf.fit(X_train, y_train) # KNN 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pred = log_clf.predict(X_test) # 로지스틱 회귀 모델로 예측\n",
    "knn_pred = knn_clf.predict(X_test) # KNN 모델로 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vot_clf = VotingClassifier(estimators = [('log', log_clf), ('knn', knn_clf)], voting = 'soft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting 방식에는 크게 'hard' 방식과 'soft' 방식이 있습니다. 먼저 'hard'는 '다수결' 원칙이라고 할 수 있겠는데요.<br> 실제 예측한 'class' 값들을 기준으로 많이 분류된 'class'를 최종 예측값으로 결정하게 됩니다.<br> 반면 'soft' 방식은 'class'별 예측 결과 확률 값을 기준으로 분류기들의 확률값을 평균을 내어 이 중 가장 높은 확률을 가진 'class'를 최종 결과 값으로 선정하게 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('log', LogisticRegression()),\n",
       "                             ('knn', KNeighborsClassifier())],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vot_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vot_pred = vot_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression모델의 정확도는 94.7368%\n",
      "KNeighborsClassifier모델의 정확도는 95.61%\n",
      "VotingClassifier모델의 정확도는 97.37%\n"
     ]
    }
   ],
   "source": [
    "print(f'{log_clf.__class__.__name__}모델의 정확도는 {np.round(accuracy_score(y_test, log_pred), 6) * 100}%')\n",
    "print(f'{knn_clf.__class__.__name__}모델의 정확도는 {np.round(accuracy_score(y_test, knn_pred), 4) * 100}%')\n",
    "print(f'{vot_clf.__class__.__name__}모델의 정확도는 {np.round(accuracy_score(y_test, vot_pred), 4) * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression의 정확도는 94.74%를 기록했고 KNN 모델의 정확도는 95.61%를 기록했습니다.<br>\n",
    "Voting 방식의 경우는 훨씬 높은 97.37%의 정확도를 보였습니다.<br>\n",
    "항상 Voting 방식의 모델이 성능이 우수하다고는 장담할 수 없습니다. 데이터의 형태와 분포 및 관련Task 등 다양한 요건들을 고려하여 여러 시도를 해보는 과정중의 일부로 접근해야 할 것입니다.\n",
    "***\n",
    "이번에는 RandomForest 알고리즘에 대해 알아보도록 하겠습니다. 앞서 배깅은 같은 알고리즘 내에서 데이터를 중복 허용의 샘플링을 통해 학습하는 것이라고 했습니다. 배깅의 대표적인 알고리즘으로는 RandomForest가 있는데 Chapter2에서 살펴본 결정트리 알고리즘을 기반으로 구성되어있습니다. 여러개의 결정트리 분류기를 중복 허용된 데이터 세트를 활용하는 알고리즘이 RandomForest라고 할 수 있겠습니다. 바로 실습과 함께 더 알아보도록 하죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state = 1212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=1212)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier모델의 정확도는 98.25%\n"
     ]
    }
   ],
   "source": [
    "print(f'{rf_clf.__class__.__name__}모델의 정확도는 {np.round(accuracy_score(y_test, rf_pred), 4) * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 사용한 LogisticRegression과 KNN 모델 보다 훨씬 더 높은 정확도를 기록했습니다. 심지어 두 모델을 Voting한 모델의 정확도 보다도 더 좋은 것을 알 수 있습니다.\n",
    "***\n",
    "이번에는 RandomForest의 하이퍼 파라미터를 살펴보겠습니다. 크게 아래의 파라미터들을 중요하다고 볼 수 있습니다.\n",
    "\n",
    "- n_estimatros : 결정 트리 개수 / default = 10 / 높을수록 성능이 높아지나 과적합 및 오랜 수행 시간 소요\n",
    "\n",
    "- max_depth : 결정트리와 마찬가지로 트리의 깊이\n",
    "\n",
    "- min_samples_leaf, min_samples_split 등등\n",
    "***\n",
    "사이킷런의 GridSearchCV를 통해 하이퍼 파라미터 탐색을 해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'n_estimators' : [30, 50 ,150, 300, 500], 'max_depth' : [3,4,5,6,7], 'min_samples_leaf' : [2,4,8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4,\n",
       "             estimator=RandomForestClassifier(n_jobs=-1, random_state=1212),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'min_samples_leaf': [2, 4, 8],\n",
       "                         'n_estimators': [30, 50, 150, 300, 500]})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(random_state = 1212, n_jobs = -1)\n",
    "grid_rf = GridSearchCV(rf_clf, param, cv = 4, n_jobs = -1)\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 하이퍼 파라미터는 {'max_depth': 7, 'min_samples_leaf': 2, 'n_estimators': 500}\n",
      "최고 정확도는 96.48%\n"
     ]
    }
   ],
   "source": [
    "print('최종 하이퍼 파라미터는', grid_rf.best_params_)\n",
    "print(f'최고 정확도는 {np.round(grid_rf.best_score_, 4) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=7, min_samples_leaf=2, n_estimators=500,\n",
       "                       random_state=1212)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rf_clf = RandomForestClassifier(**grid_rf.best_params_, random_state = 1212)\n",
    "final_rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rf_pred = final_rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier모델의 정확도는 96.49%\n"
     ]
    }
   ],
   "source": [
    "print(f'{final_rf_clf.__class__.__name__}모델의 정확도는 {np.round(accuracy_score(y_test, final_rf_pred), 4) * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼 파라미터 튜닝을 실행했지만 튜닝 전 모델의 정확도가 더 높았습니다. 충분한 컴퓨팅 자원과 시간이 있다면 더 많은 파라미터 조합을 탐색하여 보다 좋은 성능을 기록할 수 있을 것입니다.<br>\n",
    "\n",
    "지금까지 앙상블 기법 중 하나인 RandomForest에 대해서 알아보았는데요. 다음 챕터에서는 부스팅 계열의 알고리즘에 대해서 알아보도록 하겠습니다. 감사합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
