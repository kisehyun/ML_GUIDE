{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0483ecdd",
   "metadata": {},
   "source": [
    "# 텍스트 분석\n",
    "***\n",
    "### NLP와 텍스트 분석은 큰 의미적 차이는 없지만 굳이 구분을 하면 NLP는 기계가 인간의 언어를 이해하고 해석하는 데 초점을 둔 반면 텍스트 마이닝이라고 불리는 텍스트 분석은 텍스트에서 의미 있는 정보를 추출하는 것에 중점을 두고 있다.\n",
    "\n",
    "### 개인적인 생각에는 NLP가 텍스트 분석을 포함하고 있고 NLP 기술이 텍스트 분석의 발전을 가져오는게 아닌가 생각한다.\n",
    "\n",
    "### 이 책에서 텍스트 분석은 크게 4가지 종류라고 한다.\n",
    "- **텍스트 분류(text classification)** : 문서 혹은 텍스트가 특정 분류 혹은 카테고리에 속하는 것을 예측.\n",
    "- **감성 분석(sentiment analysis)** : 텍스트에 나타난 감정/판단/믿음/의견 등의 주관적인 요소를 분석하는 기법.\n",
    "- **텍스트 요약(summarization)** : 텍스트 내에서 중요한 주제나 중심 사상을 추출하는 기법\n",
    "- **텍스트 군집화(clustering)와 유사도 측정** : 비슷한 유형의 문서에 대해 군집화를 수행하는 기법\n",
    "\n",
    "***\n",
    "## 텍스트 분석 이해\n",
    "\n",
    "### 텍스트 분석은 텍스트 데이터를 수치형 feature로 변환하고 추출된 feature에 의미 있는 값을 부여하는 것이 중요하다. 텍스트를 단어의 조합의 형태인 벡터로 표현을 하는데 이를 feature vectorization 또는 featrue extraction이라고 한다.\n",
    "\n",
    "### 대표적인 python nlp 패키지는 아래와 같다.\n",
    "- **NLTK** : 가장 대표적인 패키지로 대량의 데이터 기반에서는 제대로 활용되지 못한다고 한다.\n",
    "- **Gensim** : 토픽 모델링 분야에서 가장 뛰어난 패키지로 word2vec, doc2vec 등이 있다.\n",
    "- **SpaCy** : 뛰어난 성능으로 최근 가장 주목을 받는 패키지이다.\n",
    "***\n",
    "\n",
    "### 텍스트 전처리 - 정규화\n",
    "\n",
    "#### 텍스트 정규화는 ML 알고리즘이나 NLP 애플리케이션에 input으로 사용하기 위해 클렌징, 정제, 토큰화, 어근화 등의 사전 작업을 수행하는 것을 의미한다.\n",
    "\n",
    "#### **Claening** : 불필요한 문자, 기호 등을 사전에 제거하는 작업으로 예를 들어 HTML, XML 태그나 특정 기호 등을 사전에 제거하는 것이다.\n",
    "#### **Tokenization** : 텍스트를 분리하는 것으로 문장 토큰화, 단어 토큰화 등이 있다.\n",
    "***\n",
    "**1. Sentence Tokenization** : 문장의 마침표, 개행문자 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e630e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54ae7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "You can see it out your window or on your television. \\\n",
    "You feel it when you go to work, or go to church or pay your taxes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e45c7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d620eec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type = <class 'list'>, lehgth = 3\n"
     ]
    }
   ],
   "source": [
    "print(f'type = {type(sentences)}, lehgth = {len(sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c6d6819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Matrix is everywhere its all around us, here even in this room.',\n",
       " 'You can see it out your window or on your television.',\n",
       " 'You feel it when you go to work, or go to church or pay your taxes']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1acfa",
   "metadata": {},
   "source": [
    "토크나이징 후 반환된 값은 문자열로 구성된 리스트이며 3개의 문장으로 이루어져 있다.\n",
    "\n",
    "**2. Word Tokenization** : 문장을 단어로 토큰화하는 것으로 공백, 콤마, 마침표, 개행문자 뿐만 아니라 정규 표현식을 이용해 다양한 유형으로 토큰화를 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3de306cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b65fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The Matrix is everywhere its all around us, here even in this room'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b22d2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cbc842b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type = <class 'list'>, lehgth = 14\n"
     ]
    }
   ],
   "source": [
    "print(f'type = {type(words)}, lehgth = {len(words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f97c1a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Matrix',\n",
       " 'is',\n",
       " 'everywhere',\n",
       " 'its',\n",
       " 'all',\n",
       " 'around',\n",
       " 'us',\n",
       " ',',\n",
       " 'here',\n",
       " 'even',\n",
       " 'in',\n",
       " 'this',\n",
       " 'room']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5de24",
   "metadata": {},
   "source": [
    "문장내에서 공백을 기준으로 단어 단위의 토큰화가 된 것을 알 수 있다.\n",
    "***\n",
    "- **Stopwords** : 스톱 워드는 분석에 큰 의미가 없는 단어를 지칭하는데 영어에서 is, the, a, will 등의 문맥적으로 의미가 크지 않은 단어를 뜻한다. \n",
    "\n",
    "\n",
    "\n",
    "nltk 패키지에 영어 stopwords가 존재하기 때문에 이를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35914259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kisehyun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33511356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "631b5dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dffff733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수 : 179\n"
     ]
    }
   ],
   "source": [
    "print(f'영어 stop words 개수 : {len(sw)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4a07d36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b58e4",
   "metadata": {},
   "source": [
    "토큰화 된 텍스트 중에서 stopwords가 아닌 텍스트만 filering하여 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f6e971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for tokenized in all_tokens :\n",
    "    filtered_words = []\n",
    "    \n",
    "    for token in tokenized :\n",
    "        \n",
    "        word = token.lower()\n",
    "        \n",
    "        if word not in sw :\n",
    "            filtered_words.append(word)\n",
    "            \n",
    "    tokens.append(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30367eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'],\n",
       " ['see', 'window', 'television', '.'],\n",
       " ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a0112",
   "metadata": {},
   "source": [
    "stopwords는 제거되었지만 문장 부호가 남아있는 것을 알 수 있다.\n",
    "***\n",
    "\n",
    "#### **Stemming & Lemmatization**\n",
    "\n",
    "두 기능은 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 것이다. 일반적으로 Lemmatization이 Stemming 보다 정교하며 의미론적인 기반에서 단어의 원형을 찾는다. Stemming은 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용해 일부 철자가 훼손된 어근 단어를 추출하는 경향이 있다. 반면 Lemmatization은 품사와 같은 문법적 요소와 의미적인 부분을 감안해 정확한 철자의 어근 단어를 찾아준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b0ef22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10f2b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d57d610f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work, work, work\n",
      "amus, amus, amus\n",
      "happy, happy\n",
      "fant, fanciest\n"
     ]
    }
   ],
   "source": [
    "print(f\"{stemmer.stem('working')}, {stemmer.stem('works')}, {stemmer.stem('worked')}\")\n",
    "print(f\"{stemmer.stem('amusing')}, {stemmer.stem('amuses')}, {stemmer.stem('amused')}\")\n",
    "print(f\"{stemmer.stem('happies')}, {stemmer.stem('happier')}\")\n",
    "print(f\"{stemmer.stem('fancier')}, {stemmer.stem('fanciest')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f52be86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6e5e40b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4b797cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work, work, work\n",
      "amuse, amuse, amuse\n",
      "happy, happy\n",
      "fancy, fancy\n"
     ]
    }
   ],
   "source": [
    "print(f\"{lemma.lemmatize('working', 'v')}, {lemma.lemmatize('works', 'v')}, {lemma.lemmatize('worked', 'v')}\")\n",
    "print(f\"{lemma.lemmatize('amusing', 'v')}, {lemma.lemmatize('amuses', 'v')}, {lemma.lemmatize('amused', 'v')}\")\n",
    "print(f\"{lemma.lemmatize('happiest', 'a')}, {lemma.lemmatize('happier', 'a')}\")\n",
    "print(f\"{lemma.lemmatize('fancier', 'a')}, {lemma.lemmatize('fanciest', 'a')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a42508",
   "metadata": {},
   "source": [
    "stem 방식 보다 lemma 방식이 어근 원형을 잘 탐색하는 것을 알 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
